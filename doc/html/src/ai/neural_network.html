<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.ai.neural_network API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.ai.neural_network</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
np.set_printoptions(suppress=True)

import json
import progressbar as pb

# https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-multi-class-classification/

class NeuralNetwork:
    &#39;&#39;&#39;Main class for the AI. Unless it is restarted, it reads its weights and biases from data.json.&#39;&#39;&#39;

    __HIDDEN_LAYER1_NODE_NB = 32

    def __init__(self) -&gt; None:
        self.__weights, self.__bias = dict(), dict()
        self.__epochs = 20000
        self.__lr = 0.05
        self.__nb_inputs = 7
    
    def use_saved_data(self) -&gt; None:
        &#39;&#39;&#39;Uses previously saved data&#39;&#39;&#39;
        self.load_data()
    
    def use_new_data(self) -&gt; None:
        &#39;&#39;&#39;Restarts from scratch&#39;&#39;&#39;
        self.__init_weights()
        self.__init_bias()

    def change_epochs(self, nb_of_epochs) -&gt; None:
        &#39;&#39;&#39;Ajusts the number of epochs&#39;&#39;&#39;
        self.__epochs = nb_of_epochs

    def change_learning_rate(self, learning_rate) -&gt; None:
        &#39;&#39;&#39;Changes the learning rate&#39;&#39;&#39;
        self.__lr = learning_rate

    def change_nb_inputs(self, nb_inputs) -&gt; None:
        &#39;&#39;&#39;Changes the number of inputs&#39;&#39;&#39;
        self.__nb_inputs = nb_inputs

    def __init_weights(self) -&gt; None:
        self.__weights[&#39;wh1&#39;] = np.random.rand(self.__nb_inputs, self.__HIDDEN_LAYER1_NODE_NB)
        self.__weights[&#39;wo&#39;] = np.random.rand(self.__HIDDEN_LAYER1_NODE_NB, 26)

    def __init_bias(self) -&gt; None:
        self.__bias[&#39;bh1&#39;] = np.random.rand(self.__HIDDEN_LAYER1_NODE_NB)
        self.__bias[&#39;bo&#39;] = np.random.rand(26)

    def __softmax(self, A, dimension=1) -&gt; float:
        expA = np.exp(A)
        return expA / expA.sum(axis=dimension-1, keepdims=True)

    def __sigmoid(self, x) -&gt; float:
        return 1/(1+np.exp(-x))

    def __sigmoid_der(self, x) -&gt; float:
        return self.__sigmoid(x)*(1-self.__sigmoid(x))

    def train(self, feature_set, labels) -&gt; None:
        &#39;&#39;&#39;
        Training function for the AI.
        Consists of two parts: feedforward and backpropagation.

        The feedforward first sends the inputs to the hidden layers&#39; nodes by calculating
        the dot product of the inputs and the nodes&#39; weights and adds the bias.
        It then passes the result into an activation function: the sigmoid function, 
        which squashes input values between 1 and 0.

        The AI would learn without the backpropagation, which is the function that updates
        the weights and biases for the AI to be more precise. First, it calculates the cost
        of the predictions by finding the difference between the predicted output and the 
        actual output. Then it updates the weights and biases by finding the minima. To find
        the minima, it uses a gradient decent that find the partial derivative of the cost 
        function with respect to each weight and bias and subtract the result from the existing 
        weight values to get the new weight values.

        For each epochs, the training function will run the feedforward and the backpropagation,
        updating the weights and biases to be more precise in the predictions.

        It is explained in depth here: https://stackabuse.com/creating-a-neural-network-from-scratch-in-python/
        &#39;&#39;&#39;
        bar = pb.ProgressBar(maxval=self.__epochs, \
            widgets=[pb.Bar(&#39;=&#39;, &#39;Training: [&#39;, &#39;]&#39;), \
            &#39; &#39;, pb.Counter(), f&#39;/{self.__epochs}&#39;])
        bar.start()
        for epoch in range(self.__epochs):
            nodes_h1, activation_h1, activation_out = self.__feedforward(feature_set)
            self.__back_propagation(feature_set, labels, nodes_h1, activation_h1, activation_out)
            bar.update(epoch+1)
        bar.finish()
    
    def __feedforward(self, feature_set) -&gt; np.array:
        nodes_h1 = np.dot(feature_set, self.__weights[&#39;wh1&#39;]) + self.__bias[&#39;bh1&#39;]
        activation_h1 = self.__sigmoid(nodes_h1)

        nodes_out = np.dot(activation_h1, self.__weights[&#39;wo&#39;]) + self.__bias[&#39;bo&#39;]
        activation_out = self.__softmax(nodes_out, dimension=2)
        return nodes_h1,activation_h1,activation_out

    def __back_propagation(self, feature_set, labels, nodes_h1, activation_h1, activation_out) -&gt; None:
        dcost_dzo, dcost_wo, dcost_bo = self.__bp_phase1(labels, activation_h1, activation_out)
        dcost_wh, dcost_bh = self.__bp_phase2(feature_set, nodes_h1, dcost_dzo)
        self.__update(dcost_wo, dcost_bo, dcost_wh, dcost_bh)

    def __bp_phase1(self, labels, activation_h1, activation_out) -&gt; float:
        dcost_dzo = activation_out - labels
        dzo_dwo = activation_h1

        dcost_wo = np.dot(dzo_dwo.T, dcost_dzo)

        dcost_bo = dcost_dzo
        return dcost_dzo,dcost_wo,dcost_bo

    def __bp_phase2(self, feature_set, nodes_h1, dcost_dzo) -&gt; float:
        dzo_dah = self.__weights[&#39;wo&#39;]
        dcost_dah = np.dot(dcost_dzo , dzo_dah.T)
        dah_dzh = self.__sigmoid_der(nodes_h1)
        dzh_dwh = feature_set
        dcost_wh = np.dot(dzh_dwh.T, dah_dzh * dcost_dah)

        dcost_bh = dcost_dah * dah_dzh
        return dcost_wh,dcost_bh

    def __update(self, dcost_wo, dcost_bo, dcost_wh, dcost_bh) -&gt; None:
        self.__weights[&#39;wh1&#39;] -= self.__lr * dcost_wh
        self.__bias[&#39;bh1&#39;] -= self.__lr * dcost_bh.sum(axis=0)

        self.__weights[&#39;wo&#39;] -= self.__lr * dcost_wo
        self.__bias[&#39;bo&#39;] -= self.__lr * dcost_bo.sum(axis=0)
    
    def predict(self, single_point) -&gt; np.array:
        &#39;&#39;&#39;
        Predicts the single_point value between the choices using the dot product and
        the softmax activation function.
        &#39;&#39;&#39;
        pass1 = self.__sigmoid(np.dot(single_point, self.__weights[&#39;wh1&#39;]) + self.__bias[&#39;bh1&#39;])
        return self.__softmax(np.dot(pass1, self.__weights[&#39;wo&#39;]) + self.__bias[&#39;bo&#39;])

    def save_data(self) -&gt; None:
        &#39;&#39;&#39;Writes the weights and biases into the data.json file&#39;&#39;&#39;
        json_weights = {}
        json_weights[&#39;wh1&#39;] = self.__weights[&#39;wh1&#39;].tolist()
        json_weights[&#39;wo&#39;] = self.__weights[&#39;wo&#39;].tolist()

        json_bias = {}
        json_bias[&#39;bh1&#39;] = self.__bias[&#39;bh1&#39;].tolist()
        json_bias[&#39;bo&#39;] = self.__bias[&#39;bo&#39;].tolist()

        data = {&#39;weights&#39;: json_weights, &#39;bias&#39;: json_bias}
        
        with open(&#39;src/ai/data/data.json&#39;, &#39;w+&#39;, encoding=&#39;utf-8&#39;) as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

    def load_data(self) -&gt; None:
        &#39;&#39;&#39;Reads the weights and biases from the data.json file&#39;&#39;&#39;
        with open(&#39;src/ai/data/data.json&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
            data = json.load(f)

            json_weights = data[&#39;weights&#39;]
            self.__weights[&#39;wh1&#39;] = np.array(json_weights[&#39;wh1&#39;])
            self.__weights[&#39;wo&#39;] = np.array(json_weights[&#39;wo&#39;])

            json_bias = data[&#39;bias&#39;]
            self.__bias[&#39;bh1&#39;] = np.array(json_bias[&#39;bh1&#39;])
            self.__bias[&#39;bo&#39;] = np.array(json_bias[&#39;bo&#39;])</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.ai.neural_network.NeuralNetwork"><code class="flex name class">
<span>class <span class="ident">NeuralNetwork</span></span>
</code></dt>
<dd>
<div class="desc"><p>Main class for the AI. Unless it is restarted, it reads its weights and biases from data.json.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NeuralNetwork:
    &#39;&#39;&#39;Main class for the AI. Unless it is restarted, it reads its weights and biases from data.json.&#39;&#39;&#39;

    __HIDDEN_LAYER1_NODE_NB = 32

    def __init__(self) -&gt; None:
        self.__weights, self.__bias = dict(), dict()
        self.__epochs = 20000
        self.__lr = 0.05
        self.__nb_inputs = 7
    
    def use_saved_data(self) -&gt; None:
        &#39;&#39;&#39;Uses previously saved data&#39;&#39;&#39;
        self.load_data()
    
    def use_new_data(self) -&gt; None:
        &#39;&#39;&#39;Restarts from scratch&#39;&#39;&#39;
        self.__init_weights()
        self.__init_bias()

    def change_epochs(self, nb_of_epochs) -&gt; None:
        &#39;&#39;&#39;Ajusts the number of epochs&#39;&#39;&#39;
        self.__epochs = nb_of_epochs

    def change_learning_rate(self, learning_rate) -&gt; None:
        &#39;&#39;&#39;Changes the learning rate&#39;&#39;&#39;
        self.__lr = learning_rate

    def change_nb_inputs(self, nb_inputs) -&gt; None:
        &#39;&#39;&#39;Changes the number of inputs&#39;&#39;&#39;
        self.__nb_inputs = nb_inputs

    def __init_weights(self) -&gt; None:
        self.__weights[&#39;wh1&#39;] = np.random.rand(self.__nb_inputs, self.__HIDDEN_LAYER1_NODE_NB)
        self.__weights[&#39;wo&#39;] = np.random.rand(self.__HIDDEN_LAYER1_NODE_NB, 26)

    def __init_bias(self) -&gt; None:
        self.__bias[&#39;bh1&#39;] = np.random.rand(self.__HIDDEN_LAYER1_NODE_NB)
        self.__bias[&#39;bo&#39;] = np.random.rand(26)

    def __softmax(self, A, dimension=1) -&gt; float:
        expA = np.exp(A)
        return expA / expA.sum(axis=dimension-1, keepdims=True)

    def __sigmoid(self, x) -&gt; float:
        return 1/(1+np.exp(-x))

    def __sigmoid_der(self, x) -&gt; float:
        return self.__sigmoid(x)*(1-self.__sigmoid(x))

    def train(self, feature_set, labels) -&gt; None:
        &#39;&#39;&#39;
        Training function for the AI.
        Consists of two parts: feedforward and backpropagation.

        The feedforward first sends the inputs to the hidden layers&#39; nodes by calculating
        the dot product of the inputs and the nodes&#39; weights and adds the bias.
        It then passes the result into an activation function: the sigmoid function, 
        which squashes input values between 1 and 0.

        The AI would learn without the backpropagation, which is the function that updates
        the weights and biases for the AI to be more precise. First, it calculates the cost
        of the predictions by finding the difference between the predicted output and the 
        actual output. Then it updates the weights and biases by finding the minima. To find
        the minima, it uses a gradient decent that find the partial derivative of the cost 
        function with respect to each weight and bias and subtract the result from the existing 
        weight values to get the new weight values.

        For each epochs, the training function will run the feedforward and the backpropagation,
        updating the weights and biases to be more precise in the predictions.

        It is explained in depth here: https://stackabuse.com/creating-a-neural-network-from-scratch-in-python/
        &#39;&#39;&#39;
        bar = pb.ProgressBar(maxval=self.__epochs, \
            widgets=[pb.Bar(&#39;=&#39;, &#39;Training: [&#39;, &#39;]&#39;), \
            &#39; &#39;, pb.Counter(), f&#39;/{self.__epochs}&#39;])
        bar.start()
        for epoch in range(self.__epochs):
            nodes_h1, activation_h1, activation_out = self.__feedforward(feature_set)
            self.__back_propagation(feature_set, labels, nodes_h1, activation_h1, activation_out)
            bar.update(epoch+1)
        bar.finish()
    
    def __feedforward(self, feature_set) -&gt; np.array:
        nodes_h1 = np.dot(feature_set, self.__weights[&#39;wh1&#39;]) + self.__bias[&#39;bh1&#39;]
        activation_h1 = self.__sigmoid(nodes_h1)

        nodes_out = np.dot(activation_h1, self.__weights[&#39;wo&#39;]) + self.__bias[&#39;bo&#39;]
        activation_out = self.__softmax(nodes_out, dimension=2)
        return nodes_h1,activation_h1,activation_out

    def __back_propagation(self, feature_set, labels, nodes_h1, activation_h1, activation_out) -&gt; None:
        dcost_dzo, dcost_wo, dcost_bo = self.__bp_phase1(labels, activation_h1, activation_out)
        dcost_wh, dcost_bh = self.__bp_phase2(feature_set, nodes_h1, dcost_dzo)
        self.__update(dcost_wo, dcost_bo, dcost_wh, dcost_bh)

    def __bp_phase1(self, labels, activation_h1, activation_out) -&gt; float:
        dcost_dzo = activation_out - labels
        dzo_dwo = activation_h1

        dcost_wo = np.dot(dzo_dwo.T, dcost_dzo)

        dcost_bo = dcost_dzo
        return dcost_dzo,dcost_wo,dcost_bo

    def __bp_phase2(self, feature_set, nodes_h1, dcost_dzo) -&gt; float:
        dzo_dah = self.__weights[&#39;wo&#39;]
        dcost_dah = np.dot(dcost_dzo , dzo_dah.T)
        dah_dzh = self.__sigmoid_der(nodes_h1)
        dzh_dwh = feature_set
        dcost_wh = np.dot(dzh_dwh.T, dah_dzh * dcost_dah)

        dcost_bh = dcost_dah * dah_dzh
        return dcost_wh,dcost_bh

    def __update(self, dcost_wo, dcost_bo, dcost_wh, dcost_bh) -&gt; None:
        self.__weights[&#39;wh1&#39;] -= self.__lr * dcost_wh
        self.__bias[&#39;bh1&#39;] -= self.__lr * dcost_bh.sum(axis=0)

        self.__weights[&#39;wo&#39;] -= self.__lr * dcost_wo
        self.__bias[&#39;bo&#39;] -= self.__lr * dcost_bo.sum(axis=0)
    
    def predict(self, single_point) -&gt; np.array:
        &#39;&#39;&#39;
        Predicts the single_point value between the choices using the dot product and
        the softmax activation function.
        &#39;&#39;&#39;
        pass1 = self.__sigmoid(np.dot(single_point, self.__weights[&#39;wh1&#39;]) + self.__bias[&#39;bh1&#39;])
        return self.__softmax(np.dot(pass1, self.__weights[&#39;wo&#39;]) + self.__bias[&#39;bo&#39;])

    def save_data(self) -&gt; None:
        &#39;&#39;&#39;Writes the weights and biases into the data.json file&#39;&#39;&#39;
        json_weights = {}
        json_weights[&#39;wh1&#39;] = self.__weights[&#39;wh1&#39;].tolist()
        json_weights[&#39;wo&#39;] = self.__weights[&#39;wo&#39;].tolist()

        json_bias = {}
        json_bias[&#39;bh1&#39;] = self.__bias[&#39;bh1&#39;].tolist()
        json_bias[&#39;bo&#39;] = self.__bias[&#39;bo&#39;].tolist()

        data = {&#39;weights&#39;: json_weights, &#39;bias&#39;: json_bias}
        
        with open(&#39;src/ai/data/data.json&#39;, &#39;w+&#39;, encoding=&#39;utf-8&#39;) as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

    def load_data(self) -&gt; None:
        &#39;&#39;&#39;Reads the weights and biases from the data.json file&#39;&#39;&#39;
        with open(&#39;src/ai/data/data.json&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
            data = json.load(f)

            json_weights = data[&#39;weights&#39;]
            self.__weights[&#39;wh1&#39;] = np.array(json_weights[&#39;wh1&#39;])
            self.__weights[&#39;wo&#39;] = np.array(json_weights[&#39;wo&#39;])

            json_bias = data[&#39;bias&#39;]
            self.__bias[&#39;bh1&#39;] = np.array(json_bias[&#39;bh1&#39;])
            self.__bias[&#39;bo&#39;] = np.array(json_bias[&#39;bo&#39;])</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="src.ai.neural_network.NeuralNetwork.change_epochs"><code class="name flex">
<span>def <span class="ident">change_epochs</span></span>(<span>self, nb_of_epochs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Ajusts the number of epochs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def change_epochs(self, nb_of_epochs) -&gt; None:
    &#39;&#39;&#39;Ajusts the number of epochs&#39;&#39;&#39;
    self.__epochs = nb_of_epochs</code></pre>
</details>
</dd>
<dt id="src.ai.neural_network.NeuralNetwork.change_learning_rate"><code class="name flex">
<span>def <span class="ident">change_learning_rate</span></span>(<span>self, learning_rate) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Changes the learning rate</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def change_learning_rate(self, learning_rate) -&gt; None:
    &#39;&#39;&#39;Changes the learning rate&#39;&#39;&#39;
    self.__lr = learning_rate</code></pre>
</details>
</dd>
<dt id="src.ai.neural_network.NeuralNetwork.change_nb_inputs"><code class="name flex">
<span>def <span class="ident">change_nb_inputs</span></span>(<span>self, nb_inputs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Changes the number of inputs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def change_nb_inputs(self, nb_inputs) -&gt; None:
    &#39;&#39;&#39;Changes the number of inputs&#39;&#39;&#39;
    self.__nb_inputs = nb_inputs</code></pre>
</details>
</dd>
<dt id="src.ai.neural_network.NeuralNetwork.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Reads the weights and biases from the data.json file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(self) -&gt; None:
    &#39;&#39;&#39;Reads the weights and biases from the data.json file&#39;&#39;&#39;
    with open(&#39;src/ai/data/data.json&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f:
        data = json.load(f)

        json_weights = data[&#39;weights&#39;]
        self.__weights[&#39;wh1&#39;] = np.array(json_weights[&#39;wh1&#39;])
        self.__weights[&#39;wo&#39;] = np.array(json_weights[&#39;wo&#39;])

        json_bias = data[&#39;bias&#39;]
        self.__bias[&#39;bh1&#39;] = np.array(json_bias[&#39;bh1&#39;])
        self.__bias[&#39;bo&#39;] = np.array(json_bias[&#39;bo&#39;])</code></pre>
</details>
</dd>
<dt id="src.ai.neural_network.NeuralNetwork.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, single_point) ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Predicts the single_point value between the choices using the dot product and
the softmax activation function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, single_point) -&gt; np.array:
    &#39;&#39;&#39;
    Predicts the single_point value between the choices using the dot product and
    the softmax activation function.
    &#39;&#39;&#39;
    pass1 = self.__sigmoid(np.dot(single_point, self.__weights[&#39;wh1&#39;]) + self.__bias[&#39;bh1&#39;])
    return self.__softmax(np.dot(pass1, self.__weights[&#39;wo&#39;]) + self.__bias[&#39;bo&#39;])</code></pre>
</details>
</dd>
<dt id="src.ai.neural_network.NeuralNetwork.save_data"><code class="name flex">
<span>def <span class="ident">save_data</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Writes the weights and biases into the data.json file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_data(self) -&gt; None:
    &#39;&#39;&#39;Writes the weights and biases into the data.json file&#39;&#39;&#39;
    json_weights = {}
    json_weights[&#39;wh1&#39;] = self.__weights[&#39;wh1&#39;].tolist()
    json_weights[&#39;wo&#39;] = self.__weights[&#39;wo&#39;].tolist()

    json_bias = {}
    json_bias[&#39;bh1&#39;] = self.__bias[&#39;bh1&#39;].tolist()
    json_bias[&#39;bo&#39;] = self.__bias[&#39;bo&#39;].tolist()

    data = {&#39;weights&#39;: json_weights, &#39;bias&#39;: json_bias}
    
    with open(&#39;src/ai/data/data.json&#39;, &#39;w+&#39;, encoding=&#39;utf-8&#39;) as f:
        json.dump(data, f, ensure_ascii=False, indent=4)</code></pre>
</details>
</dd>
<dt id="src.ai.neural_network.NeuralNetwork.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, feature_set, labels) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Training function for the AI.
Consists of two parts: feedforward and backpropagation.</p>
<p>The feedforward first sends the inputs to the hidden layers' nodes by calculating
the dot product of the inputs and the nodes' weights and adds the bias.
It then passes the result into an activation function: the sigmoid function,
which squashes input values between 1 and 0.</p>
<p>The AI would learn without the backpropagation, which is the function that updates
the weights and biases for the AI to be more precise. First, it calculates the cost
of the predictions by finding the difference between the predicted output and the
actual output. Then it updates the weights and biases by finding the minima. To find
the minima, it uses a gradient decent that find the partial derivative of the cost
function with respect to each weight and bias and subtract the result from the existing
weight values to get the new weight values.</p>
<p>For each epochs, the training function will run the feedforward and the backpropagation,
updating the weights and biases to be more precise in the predictions.</p>
<p>It is explained in depth here: <a href="https://stackabuse.com/creating-a-neural-network-from-scratch-in-python/">https://stackabuse.com/creating-a-neural-network-from-scratch-in-python/</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, feature_set, labels) -&gt; None:
    &#39;&#39;&#39;
    Training function for the AI.
    Consists of two parts: feedforward and backpropagation.

    The feedforward first sends the inputs to the hidden layers&#39; nodes by calculating
    the dot product of the inputs and the nodes&#39; weights and adds the bias.
    It then passes the result into an activation function: the sigmoid function, 
    which squashes input values between 1 and 0.

    The AI would learn without the backpropagation, which is the function that updates
    the weights and biases for the AI to be more precise. First, it calculates the cost
    of the predictions by finding the difference between the predicted output and the 
    actual output. Then it updates the weights and biases by finding the minima. To find
    the minima, it uses a gradient decent that find the partial derivative of the cost 
    function with respect to each weight and bias and subtract the result from the existing 
    weight values to get the new weight values.

    For each epochs, the training function will run the feedforward and the backpropagation,
    updating the weights and biases to be more precise in the predictions.

    It is explained in depth here: https://stackabuse.com/creating-a-neural-network-from-scratch-in-python/
    &#39;&#39;&#39;
    bar = pb.ProgressBar(maxval=self.__epochs, \
        widgets=[pb.Bar(&#39;=&#39;, &#39;Training: [&#39;, &#39;]&#39;), \
        &#39; &#39;, pb.Counter(), f&#39;/{self.__epochs}&#39;])
    bar.start()
    for epoch in range(self.__epochs):
        nodes_h1, activation_h1, activation_out = self.__feedforward(feature_set)
        self.__back_propagation(feature_set, labels, nodes_h1, activation_h1, activation_out)
        bar.update(epoch+1)
    bar.finish()</code></pre>
</details>
</dd>
<dt id="src.ai.neural_network.NeuralNetwork.use_new_data"><code class="name flex">
<span>def <span class="ident">use_new_data</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Restarts from scratch</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def use_new_data(self) -&gt; None:
    &#39;&#39;&#39;Restarts from scratch&#39;&#39;&#39;
    self.__init_weights()
    self.__init_bias()</code></pre>
</details>
</dd>
<dt id="src.ai.neural_network.NeuralNetwork.use_saved_data"><code class="name flex">
<span>def <span class="ident">use_saved_data</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Uses previously saved data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def use_saved_data(self) -&gt; None:
    &#39;&#39;&#39;Uses previously saved data&#39;&#39;&#39;
    self.load_data()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.ai" href="index.html">src.ai</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.ai.neural_network.NeuralNetwork" href="#src.ai.neural_network.NeuralNetwork">NeuralNetwork</a></code></h4>
<ul class="">
<li><code><a title="src.ai.neural_network.NeuralNetwork.change_epochs" href="#src.ai.neural_network.NeuralNetwork.change_epochs">change_epochs</a></code></li>
<li><code><a title="src.ai.neural_network.NeuralNetwork.change_learning_rate" href="#src.ai.neural_network.NeuralNetwork.change_learning_rate">change_learning_rate</a></code></li>
<li><code><a title="src.ai.neural_network.NeuralNetwork.change_nb_inputs" href="#src.ai.neural_network.NeuralNetwork.change_nb_inputs">change_nb_inputs</a></code></li>
<li><code><a title="src.ai.neural_network.NeuralNetwork.load_data" href="#src.ai.neural_network.NeuralNetwork.load_data">load_data</a></code></li>
<li><code><a title="src.ai.neural_network.NeuralNetwork.predict" href="#src.ai.neural_network.NeuralNetwork.predict">predict</a></code></li>
<li><code><a title="src.ai.neural_network.NeuralNetwork.save_data" href="#src.ai.neural_network.NeuralNetwork.save_data">save_data</a></code></li>
<li><code><a title="src.ai.neural_network.NeuralNetwork.train" href="#src.ai.neural_network.NeuralNetwork.train">train</a></code></li>
<li><code><a title="src.ai.neural_network.NeuralNetwork.use_new_data" href="#src.ai.neural_network.NeuralNetwork.use_new_data">use_new_data</a></code></li>
<li><code><a title="src.ai.neural_network.NeuralNetwork.use_saved_data" href="#src.ai.neural_network.NeuralNetwork.use_saved_data">use_saved_data</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>